# Hadoopé…ç½®

# à¼¼ ã¤ â—•_â—• à¼½ã¤  ğŸ‰         ğŸˆ (â—Ë‡âˆ€Ë‡â—)

### SSHè¿œç¨‹ç™»å½•é…ç½®

```shell
# vi /etc/ssh/sshd_conf
PermitEmptyPasswords  yes
PasswordAuthentication  yes
PermitRootLogin yes
# æ³¨é‡Šæ‰æœ‰ä¸ªé‡å¤çš„ no

[root] systemctl restart ssdh
```

### åˆ›å»ºå•ç‹¬ä¸ºhadoopå·¥ä½œçš„hadoopç”¨æˆ·

```shell
[root] useradd hadoop
[root] passwd hadoop
```

### å®‰è£…java

```shell
yum install java-11-openjdk java-11-openjdk-devel

# åœ¨ç³»ç»Ÿç¯å¢ƒä¸­é…ç½®javaç¯å¢ƒå˜é‡
[root] vi /etc/profile
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.14.1.1-1.el7_9.x86_64
export PATH=$PATH:$JAVA_HOME/bin

[root] resource /et/profile
```

### è§„åˆ’æœåŠ¡å™¨è§’è‰²

```shell
bigdata100  namenode             datanode     ï¼ˆä¸»èŠ‚ç‚¹ã€æ•°æ®èŠ‚ç‚¹ã€namenodeï¼‰
bigdata99  Secondarynamenode     datanode     ï¼ˆå¤‡ä»½ä¸»èŠ‚ç‚¹ã€æ•°æ®èŠ‚ç‚¹ï¼‰
bigdata98  dn1                   datanode     ï¼ˆä»èŠ‚ç‚¹ã€æ•°æ®èŠ‚ç‚¹ï¼‰
```

### åŸŸåè§£æ:warning:

```shell
[root] vi /etc/hosts
#!!!!!!!!!! ç”¨192çš„åŸŸåï¼Œåˆ«ç”¨172çš„
172.xxx.xxx.xxx		namenode	
172.xxx.xxx.xxx		Secondarynamenode
172.xxx.xxx.xxx		dn1
```

### åˆ†å‘å¯†é’¥

```shell
# ç”Ÿæˆå¯†é’¥
[root] ssh-keygen -t rsa
[root] ssh-copy-id Secondarynamenode
[root] ssh-copy-id dn1
[root] ssh-copy-id namenode # ç»™è‡ªå·±ä¸€ä¸ªå¯†é’¥ï¼Œåé¢ç”¨åˆ°
```

### é…ç½®Hadoop

```shell
# å®‰è£…wgetå’Œç”¨wgetä¸‹è½½hadoop
[root]  yum install wget
[root]	su hadoop
[hadoop] cd
[hadoop] wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz
# è§£å‹hadoopåˆ°~
[hadoop] tar -xvf hadoop-3.3.2/hadoop-3.3.2.tar.gz
# åˆ›å»ºdataæ–‡ä»¶å¤¹å­˜æ•°æ®
[hadoop ~] mkdir data


# hadoopå‘½ä»¤å­˜æ”¾åœ¨/home/hadoop/hadoop3.3.2/binä¸‹
# æŠŠè·¯å¾„åŠ å…¥åˆ°hadoopç¯å¢ƒå˜é‡ä¸­ï¼Œä»¥åä¸ç”¨å†™è·¯å¾„è°ƒå‘½ä»¤
# ç”¨æˆ·çš„ç¯å¢ƒå˜é‡ä¿®æ”¹.bashrcå³å¯
[hadoop] cd
[hadoop] vi .bashrc
export HADOOP_HOME=/home/hadoop/hadoop-3.3.2
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

[hadoop] source .bashrc
# æµ‹è¯•ä¸‹
[hadoop ~] hadoop version
# å‡ºç°ä¸‹é¢å†…å®¹å³æˆåŠŸ
#Hadoop 3.3.2
#Source code repository git@github.com:apache/hadoop.git -r #0bcb014209e219273cb6fd4152df7df713cbac61
#Compiled by chao on 2022-02-21T18:39Z
#Compiled with protoc 3.7.1
#From source with checksum 4b40fff8bb27201ba07b6fa5651217fb
#This command was run using /home/hadoop/hadoop-#3.3.2/share/hadoop/common/hadoop-common-3.3.2.jar
```

### ç¼–è¾‘hadoopé…ç½®æ–‡ä»¶

```shell
# ä½ç½® /home/hadoop/hadoop-3.3.2/etc/hadoop
[hadoop] cd /home/hadoop/hadoop-3.3.2/etc/hadoop
[hadoop] vi core-site.xml
# ä¿®æ”¹æ·»åŠ core-site.xmlå†…å®¹ï¼Œ ç¡®ä¿dataæ–‡ä»¶å¤¹å·²åˆ›å»º
<configuration>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://namenode:9000</value>
        </property>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>file:/home/hadoop/data/tmp</value>
        </property>
</configuration>


[hadoop] vi hdfs-sit.xml
# ä¿®æ”¹æ·»åŠ hdfs-sit.xmlå†…å®¹ï¼Œ ç¡®ä¿dataæ–‡ä»¶å¤¹å·²åˆ›å»º
<configuration>
        <property>
                <name>dfs.namenode.secondary.http-address</name>
                <value>Secondarynamenode:50090</value>
        </property>
        <property>
                <name>dfs.replication</name>
                <value>3</value>
        </property>
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>file:/home/hadoop/data/name</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>file:/home/hadoop/data/data</value>
        </property>
<configuration>

# ä¿®æ”¹æ·»åŠ mapred-site.xmlå†…å®¹
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.address</name>
                <value>namenode:10020</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.webapp.address</name>
                <value>namenode:19888</value>
        </property>
# ä¿®æ”¹æ·»åŠ yarn-site.xmlå†…å®¹
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>namenode</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
# ä¿®æ”¹æ·»åŠ workerså†…å®¹
# åˆ é™¤localhost
    namenode
    Secondarynamenode
    dn1
# ä¿®æ”¹hadoop-env.sh
export HADOOP_HOME=/home/hadoop/hadoop-3.3.2
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.14.1.1-1.el7_9.x86_64

```

### åˆ†å‘hadoop

```shell
[hadoop] scp -r hadoop-3.3.2 hadoop@Secordarynamenode:~/
[hadoop] scp -r hadoop-3.3.2 hadoop@dn1:~/
```

### ç»™æœ¬æœºä¸€ä¸ªssh

```shell
[hadoop] ssh-copy-id namenode
```

### å¯åŠ¨

```shell
hdfs namenode -format

start-dfs.sh
start-yarn.sh
jps
```

### æ–‡ä»¶æ“ä½œ

```shell
hadoop fs -put [from] [target]
hadoop fs -get
hadoop fs -ls /
```

### WebæŸ¥çœ‹

```shell
[master ip]:9870
```

## à¼¼ ã¤ â—•_â—• à¼½ã¤   â¡å‡ºé”™å¤šçœ‹logsï¼ï¼ï¼ï¼ï¼ï¼ 
## é…Hadoopé…éº»äº†ï¼ :cry:
---
# The end



























































































